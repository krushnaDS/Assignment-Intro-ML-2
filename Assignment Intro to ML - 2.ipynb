{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7384f216",
   "metadata": {},
   "source": [
    "##### Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n",
    "\n",
    "\n",
    "###### Overfitting\n",
    "Occurs when a model becomes too complex and memorizes the specific details and noise present in the training data instead of capturing the underlying general patterns.\n",
    "\n",
    "- Consequences:\n",
    "    - Poor performance on unseen data.\n",
    "    - Increased variance.\n",
    "   \n",
    "- To mitigate overfitting:\n",
    "    - Regularization\n",
    "    - Data augmentation\n",
    "    - Reducing model complexities\n",
    "    \n",
    "###### Underfitting\n",
    "Occurs when a model is too simple and fails to capture the underlying relationship and complexities within the training data.\n",
    "\n",
    "- Consequences\n",
    "    - Poor performance on both training and test data.\n",
    "    - High bias.\n",
    "    \n",
    "- To mitigate underfitting\n",
    "    - Increasing model complexities.\n",
    "    - Collecting more data.\n",
    "    - Feature engineering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e08383f",
   "metadata": {},
   "source": [
    "##### Q2: How can we reduce overfitting? Explain in brief.\n",
    "\n",
    "Here are some quick ways to reduce overfitting in machine learning:\n",
    "\n",
    " 1. Regularization\n",
    " - Add penalties to the model's cost function, making complex models with many parameters less favorable.\n",
    " - L1 and L2 regularization.\n",
    " \n",
    " 2. Early stopping\n",
    " - Monitors the model's performance on a separate validation set and stops training when performance on the validation set starts to degrade.\n",
    " - prevent the model from memorizing noise in the training data set.\n",
    " \n",
    " 3. Data Augmentation\n",
    " - Artificially increase the size and diversity of your training data by applying random transformations.\n",
    " - Makes model more robust to unseen variations in the data.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8333039b",
   "metadata": {},
   "source": [
    "##### Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "\n",
    "Underfitting\n",
    "Underfitting occurs in machine learning when a model is too simple and fails to capture the underlying relationships and complexities within the training data.\n",
    "\n",
    "Scenarios where Underfitting occurs:\n",
    "- Using a model that is too simple\n",
    "- limited training data\n",
    "- Incorrect feature selection\n",
    "- Insufficient training time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b46aa",
   "metadata": {},
   "source": [
    "##### Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?\n",
    "\n",
    "The bias-variance trade-off is a fundamental concept in machine learning that describes the inherent relationship between a model's complexity and its generalizability.\n",
    "\n",
    "As we increase the complexity of a model its bias typically decreases but its variance typically increases.\n",
    "\n",
    "- Relationship to model performance:\n",
    "    - High bias and low variance: The model is underfitting, leading to consistently poor performance across all data.\n",
    "    - Low bias and high variance: The model is overfitting performing well on training data but poorely on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d91244a",
   "metadata": {},
   "source": [
    "##### Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?\n",
    "\n",
    "1. Comparing training and validation error:\n",
    "Overfitting:\n",
    "If the training error is significantly lower than the validation error, it suggest the model is memorizing noise and overfitting to the training data.\n",
    "Underfitting:\n",
    "If both training and validation errors are high it could indicate the model is too simple and underfitting the data.\n",
    "\n",
    "2. Visualization technique:\n",
    "- Learning curves\n",
    "- Decision boundaries\n",
    "\n",
    "3. Performance metrics:\n",
    "- Overfitting - Accuracy,recall f1-score,mse\n",
    "- Underfitting - Look for constantly low performance across all metrics on both training and unseen data.\n",
    "\n",
    "4. Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfc2139",
   "metadata": {},
   "source": [
    "##### Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?\n",
    "\n",
    "- Bias :\n",
    "Refers to the systematic error introduced by the model's assumptions and limitations.It represents the average difference between the model's performance and the true values.\n",
    "High bias leads to Underfitting.\n",
    "Example: Imagine a model predicting house prices using only size as a feature. This model is likely to be biased because it ignores other important factors like location, amenities, and market trends. Its predictions will be consistently off the mark, regardless of the data used for training.\n",
    "\n",
    "- Variance:\n",
    "Represents the variability in the model's predictions. It reflects how much the model's predictions would change if you trained it on different training sets drawn from the same data distributions.\n",
    "High variance leads to overfitting.\n",
    "Example: Imagine a complex model that predicts spam emails by analyzing every word in the email. This model might overfit to specific phrases or wording used in the training data, leading to accurate identification of spam emails within the training set but failing to generalize well to unseen spam emails with different wording."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe3dd78",
   "metadata": {},
   "source": [
    "##### Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.\n",
    "\n",
    "Regularization is a set of techniques used to reduce overfitting in machine learning models. Overfitting occurs when a model becomes too complex and memorizes specific details and noise present in the training data instead of capturing the underlying general patterns. Regularization helps prevent this by:\n",
    "\n",
    "1. L1 Regularization(Lasso regression)\n",
    "- Adds the asolute value of the model's coefficients to the cost function.\n",
    "- This penalizes models with large coefficients, driving some coefficients to zero and effectively reducing model complexity.\n",
    "\n",
    "2. L2 Regularizatin(Ridge regression)\n",
    "- Adds square of the models coefficients to the cost function.\n",
    "- This penalizes models with large coefficients, but to a lesser extent than L1 regularization.Unlike L1 it doesn't necessarily drive coe. to zero.\n",
    "\n",
    "3. Elastic net\n",
    "- Combines L1 and L2 regularization.\n",
    "- It utilizes a hyperparameter to control the balance between L1 and L2 penalties, allowing you to control the amount of feature selection and smoothness in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49693353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
